# CNN-LSTMvsBLIP-2
 This repository contains a comparative study of deep learning architectures for Medical Visual Question Answering (Med-VQA) using the VQA-RAD dataset. The project evaluates two distinct philosophies: a specialized CNN-LSTM baseline trained from scratch and a state-of-the-art BLIP-2 Vision-Language Model evaluated in a zero-shot setting.
